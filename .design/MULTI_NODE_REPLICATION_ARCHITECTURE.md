# RustFS å¤šæœºå¤šç›˜æ•°æ®å‰¯æœ¬å¤„ç†æ¶æ„

## ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
3. [æ“¦é™¤ç¼–ç åŸç†](#æ“¦é™¤ç¼–ç åŸç†)
4. [æ•°æ®å†™å…¥æµç¨‹](#æ•°æ®å†™å…¥æµç¨‹)
5. [æ•°æ®è¯»å–æµç¨‹](#æ•°æ®è¯»å–æµç¨‹)
6. [åˆ†å¸ƒå¼åè°ƒ](#åˆ†å¸ƒå¼åè°ƒ)
7. [æ•…éšœæ¢å¤](#æ•…éšœæ¢å¤)
8. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

## æ¦‚è¿°

RustFS ä½¿ç”¨**æ“¦é™¤ç¼–ç  (Erasure Coding)** æŠ€æœ¯è€Œéä¼ ç»Ÿçš„å¤šå‰¯æœ¬å¤åˆ¶æ–¹å¼æ¥å®ç°æ•°æ®å†—ä½™å’Œé«˜å¯ç”¨æ€§ã€‚è¿™ç§æ–¹å¼åœ¨ä¿è¯æ•°æ®å¯é æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å­˜å‚¨ç©ºé—´åˆ©ç”¨ç‡ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ¯ **æ“¦é™¤ç¼–ç **: åŸºäº Reed-Solomon ç®—æ³•çš„æ•°æ®ä¿æŠ¤
- ğŸŒ **åˆ†å¸ƒå¼æ¶æ„**: æ”¯æŒè·¨èŠ‚ç‚¹æ•°æ®åˆ†å¸ƒ
- ğŸ”„ **è‡ªåŠ¨ä¿®å¤**: åå°è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤æŸåæ•°æ®
- âš¡ **SIMD ä¼˜åŒ–**: é«˜æ€§èƒ½ç¼–è§£ç å®ç°
- ğŸ“Š **çµæ´»é…ç½®**: æ”¯æŒå¤šç§æ•°æ®/æ ¡éªŒå—ç»„åˆ

### å¯¹æ¯”ä¼ ç»Ÿå‰¯æœ¬æ–¹å¼

| ç‰¹æ€§   | ä¼ ç»Ÿ 3 å‰¯æœ¬        | RustFS æ“¦é™¤ç¼–ç  (8+4) |
|------|----------------|-------------------|
| å­˜å‚¨å¼€é”€ | 3x (33.3% åˆ©ç”¨ç‡) | 1.5x (66.7% åˆ©ç”¨ç‡)  |
| å®¹é”™èƒ½åŠ› | 2 å—ç£ç›˜          | 4 å—ç£ç›˜             |
| å†™æ”¾å¤§  | 3x             | 1.5x              |
| è¯»æ€§èƒ½  | ä»ä»»æ„å‰¯æœ¬è¯»         | ä»ä»»æ„ 8 ä¸ªåˆ†ç‰‡è¯»        |
| ä¿®å¤å¼€é”€ | å¤åˆ¶å®Œæ•´æ•°æ®         | ä»…éœ€ 8 ä¸ªåˆ†ç‰‡æ•°æ®        |

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client Layer                             â”‚
â”‚                    (S3 Compatible API)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ECStore (Storage Engine)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Endpoint Server Pools                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚  Pool 0    â”‚  â”‚  Pool 1    â”‚  â”‚  Pool N    â”‚         â”‚  â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ Sets   â”‚ â”‚  â”‚ â”‚ Sets   â”‚ â”‚  â”‚ â”‚ Sets   â”‚ â”‚         â”‚  â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Erasure Coding Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            Reed-Solomon SIMD Encoder/Decoder              â”‚  â”‚
â”‚  â”‚  Data Shards: [D0, D1, D2, ..., DN]                       â”‚  â”‚
â”‚  â”‚  Parity Shards: [P0, P1, P2, ..., PM]                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“              â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1        â”‚ â”‚   Node 2        â”‚ â”‚   Node 3        â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”“  â”‚ â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”“  â”‚ â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”“  â”‚
â”‚  â”ƒ Local     â”ƒ  â”‚ â”‚  â”ƒ Local     â”ƒ  â”‚ â”‚  â”ƒ Local     â”ƒ  â”‚
â”‚  â”ƒ Disks     â”ƒ  â”‚ â”‚  â”ƒ Disks     â”ƒ  â”‚ â”‚  â”ƒ Disks     â”ƒ  â”‚
â”‚  â”—â”â”â”â”â”â”â”â”â”â”â”â”›  â”‚ â”‚  â”—â”â”â”â”â”â”â”â”â”â”â”â”›  â”‚ â”‚  â”—â”â”â”â”â”â”â”â”â”â”â”â”›  â”‚
â”‚  /mnt/disk1     â”‚ â”‚  /mnt/disk1     â”‚ â”‚  /mnt/disk1     â”‚
â”‚  /mnt/disk2     â”‚ â”‚  /mnt/disk2     â”‚ â”‚  /mnt/disk2     â”‚
â”‚  /mnt/disk3     â”‚ â”‚  /mnt/disk3     â”‚ â”‚  /mnt/disk3     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ•°æ®ç»“æ„

#### 1. Endpoint Server Pools

```rust
// ä½ç½®ï¼šcrates/ecstore/src/endpoints.rs

pub struct EndpointServerPools {
    pools: Vec<PoolEndpoints>,
}

pub struct PoolEndpoints {
    pub set_count: usize,           // Set æ•°é‡
    pub drives_per_set: usize,      // æ¯ä¸ª Set çš„ç£ç›˜æ•°
    pub endpoints: Vec<Endpoint>,   // æ‰€æœ‰ç«¯ç‚¹ï¼ˆæœ¬åœ° + è¿œç¨‹ï¼‰
}

pub struct Endpoint {
    pub scheme: String,             // http/https
    pub host: String,               // èŠ‚ç‚¹åœ°å€
    pub port: u16,                  // ç«¯å£
    pub path: String,               // ç£ç›˜è·¯å¾„
    pub is_local: bool,             // æ˜¯å¦æœ¬åœ°ç£ç›˜
    pub pool_idx: u8,               // Pool ç´¢å¼•
    pub set_idx: u8,                // Set ç´¢å¼•
    pub disk_idx: usize,            // Disk ç´¢å¼•
}
```

**ç¤ºä¾‹é…ç½®**:

```yaml
# 3 èŠ‚ç‚¹ Ã— 4 ç£ç›˜ = 12 ç£ç›˜ï¼Œé…ç½®ä¸º 8+4 æ“¦é™¤ç¼–ç 
Pool 0:
  Set 0: [N1D1, N1D2, N2D1, N2D2, N3D1, N3D2, N3D3, N3D4, N1D3, N1D4, N2D3, N2D4]
         â””â”€â”€â”€â”€â”€â”€â”€ 8 ä¸ªæ•°æ®ç›˜ â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4 ä¸ªæ ¡éªŒç›˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Sets (æ“¦é™¤ç¼–ç é›†åˆ)

```rust
// ä½ç½®ï¼šcrates/ecstore/src/sets.rs

pub struct Sets {
    pub set_disks: Vec<Arc<SetDisks>>,  // å¤šä¸ªæ“¦é™¤ç¼–ç å•å…ƒ
    pub format: FormatV3,                // æ ¼å¼é…ç½®
    pub pool_index: usize,               // Pool ç´¢å¼•
}
```

#### 3. SetDisks (æ“¦é™¤ç¼–ç å•å…ƒ)

```rust
// ä½ç½®ï¼šcrates/ecstore/src/set_disk.rs

pub struct SetDisks {
    disks: Arc<RwLock<Vec<Option<DiskStore>>>>,  // ç£ç›˜åˆ—è¡¨
    format: FormatV3,                             // æ ¼å¼é…ç½®
    pool_index: usize,                            // Pool ç´¢å¼•
    set_index: usize,                             // Set ç´¢å¼•
    lock_clients: HashMap<String, Arc<dyn LockClient>>,
}
```

#### 4. DiskStore (ç£ç›˜æŠ½è±¡)

```rust
// ä½ç½®ï¼šcrates/ecstore/src/disk/mod.rs

pub enum Disk {
    Local(LocalDiskWrapper),    // æœ¬åœ°ç£ç›˜
    Remote(RemoteDisk),         // è¿œç¨‹ç£ç›˜ï¼ˆé€šè¿‡ RPCï¼‰
}

pub struct LocalDisk {
    endpoint: Endpoint,
    path: PathBuf,              // æŒ‚è½½è·¯å¾„
    format_info: RwLock<FormatInfo>,
    metrics: Arc<DiskMetrics>,
}
```

## æ“¦é™¤ç¼–ç åŸç†

### Reed-Solomon ç®—æ³•

RustFS ä½¿ç”¨ Reed-Solomon (RS) ç¼–ç ï¼Œè¿™æ˜¯ä¸€ç§å‰å‘çº é”™ç  (FEC)ï¼Œå¹¿æ³›åº”ç”¨äºå­˜å‚¨ç³»ç»Ÿå’Œé€šä¿¡ç³»ç»Ÿã€‚

#### æ•°å­¦åŸç†

**æœ‰é™åŸŸè¿ç®—**: RS ç¼–ç åŸºäº Galois Field GF(2^8) ä¸Šçš„å¤šé¡¹å¼è¿ç®—ã€‚

**ç¼–ç è¿‡ç¨‹**:

1. å°†åŸå§‹æ•°æ®åˆ†æˆ K ä¸ªæ•°æ®å—ï¼š`D = [D0, D1, ..., D(K-1)]`
2. ç”Ÿæˆ M ä¸ªæ ¡éªŒå—ï¼š`P = [P0, P1, ..., P(M-1)]`
3. ä½¿ç”¨èŒƒå¾·è’™å¾·çŸ©é˜µæˆ–æŸ¯è¥¿çŸ©é˜µè¿›è¡Œçº¿æ€§å˜æ¢

**è§£ç è¿‡ç¨‹**:

- åªéœ€ä»»æ„ K ä¸ªå®Œæ•´å—ï¼ˆæ•°æ®å—æˆ–æ ¡éªŒå—ï¼‰å³å¯æ¢å¤åŸå§‹æ•°æ®
- æ”¯æŒæœ€å¤š M ä¸ªå—ä¸¢å¤±çš„æƒ…å†µ

#### ä»£ç å®ç°

```rust
// ä½ç½®ï¼šcrates/ecstore/src/erasure_coding/erasure.rs

pub struct Erasure {
    pub data_shards: usize,      // K: æ•°æ®åˆ†ç‰‡æ•°
    pub parity_shards: usize,    // M: æ ¡éªŒåˆ†ç‰‡æ•°
    encoder: Option<ReedSolomonEncoder>,
    pub block_size: usize,       // æ¯ä¸ªåˆ†ç‰‡çš„å—å¤§å°
}

impl Erasure {
    /// åˆ›å»ºæ“¦é™¤ç¼–ç å®ä¾‹
    pub fn new(data_shards: usize, parity_shards: usize, block_size: usize) -> Self {
        let encoder = if parity_shards > 0 {
            Some(ReedSolomonEncoder::new(data_shards, parity_shards).unwrap())
        } else {
            None
        };

        Erasure {
            data_shards,
            parity_shards,
            block_size,
            encoder,
            _id: Uuid::new_v4(),
            _buf: vec![0u8; block_size],
        }
    }

    /// è®¡ç®—å­˜å‚¨æ•ˆç‡
    pub fn data_efficiency(&self) -> f64 {
        self.data_shards as f64 / (self.data_shards + self.parity_shards) as f64
    }

    /// è®¡ç®—å•ä¸ªåˆ†ç‰‡å¤§å°
    pub fn shard_size(&self) -> usize {
        self.block_size
    }

    /// è®¡ç®—åˆ†ç‰‡æ–‡ä»¶æ€»å¤§å°
    pub fn shard_file_size(&self, data_size: usize) -> usize {
        let shard_count = (data_size + self.block_size - 1) / self.block_size;
        shard_count * self.block_size / self.data_shards
    }
}
```

### SIMD ä¼˜åŒ–

RustFS ä½¿ç”¨ `reed-solomon-simd` åº“ï¼Œåˆ©ç”¨ CPU çš„ SIMD æŒ‡ä»¤é›†åŠ é€Ÿç¼–è§£ç ï¼š

```rust
// ä½ç½®ï¼šcrates/ecstore/src/erasure_coding/erasure.rs

pub struct ReedSolomonEncoder {
    data_shards: usize,
    parity_shards: usize,
    encoder_cache: std::sync::RwLock<Option<reed_solomon_simd::ReedSolomonEncoder>>,
    decoder_cache: std::sync::RwLock<Option<reed_solomon_simd::ReedSolomonDecoder>>,
}

impl ReedSolomonEncoder {
    fn encode_with_simd(&self, shards_vec: &mut [&mut [u8]]) -> io::Result<()> {
        let shard_len = shards_vec[0].len();

        // è·å–æˆ–åˆ›å»ºç¼–ç å™¨
        let encoder = {
            let cache = self.encoder_cache.read().unwrap();
            if let Some(enc) = cache.as_ref() {
                enc.clone()
            } else {
                drop(cache);
                let mut cache = self.encoder_cache.write().unwrap();
                let enc = reed_solomon_simd::ReedSolomonEncoder::new(
                    self.data_shards,
                    self.parity_shards,
                    shard_len,
                )?;
                *cache = Some(enc.clone());
                enc
            }
        };

        // SIMD ç¼–ç 
        encoder.encode(shards_vec)?;
        Ok(())
    }
}
```

**æ€§èƒ½æå‡**:

- AVX2: 4-8x åŠ é€Ÿ
- AVX-512: 8-16x åŠ é€Ÿ
- NEON (ARM): 2-4x åŠ é€Ÿ

## æ•°æ®å†™å…¥æµç¨‹

### å®Œæ•´å†™å…¥æµç¨‹å›¾

```
Client PUT Request
       â†“
[1] ECStore::put_object
       â†“
[2] é€‰æ‹© Set (åŸºäºå¯¹è±¡åå“ˆå¸Œ)
       â†“
[3] SetDisks::put_object
       â†“
[4] åˆ›å»ºä¸´æ—¶å¯¹è±¡ UUID
       â†“
[5] è®¡ç®—å†™å…¥ä»²è£æ•°
       â†“
[6] ä¸ºæ¯ä¸ªç£ç›˜åˆ›å»º BitrotWriter
       â†“                          â”Œâ”€ Local Disk (ç›´æ¥å†™)
       â”œâ”€ Disk 0 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 1 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 2 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 3 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 4 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 5 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 6 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 7 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 8 (Parity) â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 9 (Parity) â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 10 (Parity) â”€â”€â”€â”€â”€â”€â”¼â”€ Remote Disk (RPC å†™)
       â””â”€ Disk 11 (Parity) â”€â”€â”€â”€â”€â”€â”˜
       â†“
[7] æ‰§è¡Œ Erasure ç¼–ç 
       â†“
[8] MultiWriter å¹¶è¡Œå†™å…¥æ‰€æœ‰åˆ†ç‰‡
       â†“
[9] æ£€æŸ¥å†™å…¥ä»²è£ï¼ˆè‡³å°‘ data_shards ä¸ªæˆåŠŸï¼‰
       â†“
[10] å†™å…¥å…ƒæ•°æ®åˆ°æ‰€æœ‰ç£ç›˜
       â†“
[11] æäº¤ï¼šä¸´æ—¶æ–‡ä»¶ â†’ æœ€ç»ˆä½ç½®
       â†“
[12] è¿”å› ObjectInfo
```

### å…³é”®æ­¥éª¤è¯¦è§£

#### æ­¥éª¤ 1-3: è¯·æ±‚è·¯ç”±

```rust
// ä½ç½®ï¼šcrates/ecstore/src/store.rs

#[async_trait::async_trait]
impl StorageAPI for ECStore {
    async fn put_object(&self, bucket: &str, key: &str, ...) -> Result<ObjectInfo> {
        // 1. é€‰æ‹©åˆé€‚çš„ Pool (å½“å‰ä»…æ”¯æŒå• Pool)
        let pool = &self.pools[0];
        
        // 2. åŸºäºå¯¹è±¡åå“ˆå¸Œé€‰æ‹© Set
        let set_index = self.get_set_index(key);
        let set_disks = &pool.set_disks[set_index];
        
        // 3. å§”æ‰˜ç»™ SetDisks å¤„ç†
        set_disks.put_object(bucket, key, data, opts).await
    }
}
```

#### æ­¥éª¤ 4-6: å‡†å¤‡å†™å…¥

```rust
// ä½ç½®ï¼šcrates/ecstore/src/set_disk.rs

async fn put_object(&self, bucket: &str, object: &str, ...) -> Result<ObjectInfo> {
    // 4. åˆ›å»ºä¸´æ—¶ç›®å½•å’Œ UUID
    let tmp_dir = Uuid::new_v4().to_string();
    let data_dir = Uuid::new_v4();
    let tmp_object = format!("{}/{}/part.1", tmp_dir, data_dir);

    // 5. è®¡ç®—å†™å…¥ä»²è£æ•°
    let data_drives = self.format.erasure.data_blocks;
    let parity_drives = self.format.erasure.parity_blocks;
    let mut write_quorum = data_drives - parity_drives;
    if data_drives == parity_drives {
        write_quorum += 1;  // éœ€è¦å¤šæ•°æ´¾
    }

    // 6. ä¸ºæ¯ä¸ªåœ¨çº¿ç£ç›˜åˆ›å»º writer
    let disks = self.disks.read().await;
    let shuffle_disks = Self::shuffle_disks(&disks, &fi.erasure.distribution);
    
    let mut writers = Vec::with_capacity(shuffle_disks.len());
    for disk in shuffle_disks.iter() {
        if let Some(disk) = disk && disk.is_online().await {
            let writer = create_bitrot_writer(
                is_inline_buffer,
                Some(disk),
                RUSTFS_META_TMP_BUCKET,
                &tmp_object,
                erasure.shard_file_size(data.size()),
                erasure.shard_size(),
                HashAlgorithm::HighwayHash256,
            ).await?;
            writers.push(Some(writer));
        } else {
            writers.push(None);  // ç¦»çº¿æˆ–ä¸å¯ç”¨ç£ç›˜
        }
    }
}
```

#### æ­¥éª¤ 7-8: æ“¦é™¤ç¼–ç ä¸å¹¶è¡Œå†™å…¥

```rust
// 7. åˆ›å»ºæ“¦é™¤ç¼–ç å®ä¾‹
let erasure = Arc::new(erasure_coding::Erasure::new(
    fi.erasure.data_blocks,
    fi.erasure.parity_blocks,
    fi.erasure.block_size,
));

// 8. æ‰§è¡Œç¼–ç å¹¶å¹¶è¡Œå†™å…¥
let (reader, written_size) = erasure
    .encode(data.stream, &mut writers, write_quorum)
    .await?;
```

**ç¼–ç å®ç°** (`crates/ecstore/src/erasure_coding/encode.rs`):

```rust
pub async fn encode<R>(
    self: Arc<Self>,
    reader: R,
    writers: &mut [Option<BitrotWriterWrapper>],
    write_quorum: usize,
) -> Result<(HashReader<R>, usize)>
where
    R: AsyncRead + Send + Unpin + 'static,
{
    let mut multi_writer = MultiWriter::new(writers, write_quorum);
    let mut total_written = 0;

    // æµå¼è¯»å–å’Œç¼–ç 
    let mut buffer = vec![0u8; self.block_size * self.data_shards];
    loop {
        let n = read_full(reader, &mut buffer).await?;
        if n == 0 {
            break;
        }

        // åˆ†ç‰‡å¹¶å¡«å……åˆ°å—å¤§å°
        let mut shards = self.split_data(&buffer[..n]);
        
        // Reed-Solomon ç¼–ç ç”Ÿæˆæ ¡éªŒåˆ†ç‰‡
        self.encoder.as_ref().unwrap().encode(&mut shards)?;

        // å¹¶è¡Œå†™å…¥æ‰€æœ‰åˆ†ç‰‡
        multi_writer.write(&shards).await?;
        
        total_written += n;
    }

    // ç¡®ä¿æ»¡è¶³å†™å…¥ä»²è£
    multi_writer.close().await?;

    Ok((reader, total_written))
}
```

**å¹¶è¡Œå†™å…¥** (`MultiWriter`):

```rust
pub struct MultiWriter<'a> {
    writers: &'a mut [Option<BitrotWriterWrapper>],
    write_quorum: usize,
    errs: Vec<Option<Error>>,
}

impl<'a> MultiWriter<'a> {
    pub async fn write(&mut self, shards: &[Bytes]) -> Result<()> {
        // å¹¶è¡Œå†™å…¥æ‰€æœ‰åˆ†ç‰‡åˆ°å¯¹åº”ç£ç›˜
        let mut futures = Vec::with_capacity(self.writers.len());
        
        for (i, writer_opt) in self.writers.iter_mut().enumerate() {
            let shard = shards[i].clone();
            futures.push(Self::write_shard(writer_opt, &mut self.errs[i], &shard));
        }

        join_all(futures).await;

        // æ£€æŸ¥å†™å…¥ä»²è£
        let success_count = self.errs.iter().filter(|e| e.is_none()).count();
        if success_count < self.write_quorum {
            return Err(Error::InsufficientWriteQuorum);
        }

        Ok(())
    }

    async fn write_shard(
        writer_opt: &mut Option<BitrotWriterWrapper>,
        err: &mut Option<Error>,
        shard: &Bytes,
    ) {
        match writer_opt {
            Some(writer) => {
                match writer.write(shard).await {
                    Ok(n) if n == shard.len() => *err = None,
                    Ok(_) => *err = Some(Error::ShortWrite),
                    Err(e) => *err = Some(e.into()),
                }
            }
            None => *err = Some(Error::DiskNotFound),
        }
    }
}
```

#### æ­¥éª¤ 9-11: å…ƒæ•°æ®ä¸æäº¤

```rust
// 9. æ£€æŸ¥å·²åœ¨ MultiWriter::close() ä¸­å®Œæˆ

// 10. å†™å…¥å…ƒæ•°æ®åˆ°æ‰€æœ‰ç£ç›˜
Self::write_unique_file_info(
    &disks,
    org_bucket,
    bucket,
    prefix,
    &parts_metadatas,
    write_quorum,
).await?;

// 11. æäº¤æ•°æ®ï¼šç§»åŠ¨ä¸´æ—¶æ–‡ä»¶åˆ°æœ€ç»ˆä½ç½®
self.commit_data(
    bucket,
    object,
    &tmp_dir,
    &data_dir.to_string(),
    &fi,
).await?;
```

**å…ƒæ•°æ®å†™å…¥**:

```rust
async fn write_unique_file_info(
    disks: &[Option<DiskStore>],
    org_bucket: &str,
    bucket: &str,
    prefix: &str,
    files: &[FileInfo],
    write_quorum: usize,
) -> Result<()> {
    let mut futures = Vec::with_capacity(disks.len());

    // ä¸ºæ¯ä¸ªç£ç›˜å‡†å¤‡ç‹¬ç‰¹çš„å…ƒæ•°æ®ï¼ˆåŒ…å«åˆ†ç‰‡ç´¢å¼•ç­‰ï¼‰
    for (i, disk) in disks.iter().enumerate() {
        let mut file_info = files[i].clone();
        file_info.erasure.index = i + 1;  // åˆ†ç‰‡ç´¢å¼•
        
        futures.push(async move {
            if let Some(disk) = disk {
                disk.write_metadata(org_bucket, bucket, prefix, file_info).await
            } else {
                Err(DiskError::DiskNotFound)
            }
        });
    }

    let results = join_all(futures).await;
    
    // æ£€æŸ¥å†™å…¥ä»²è£
    let success_count = results.iter().filter(|r| r.is_ok()).count();
    if success_count < write_quorum {
        return Err(Error::InsufficientWriteQuorum);
    }

    Ok(())
}
```

### å†™å…¥ä»²è£è§„åˆ™

```rust
// ä½ç½®ï¼šcrates/filemeta/src/fileinfo.rs

impl ErasureInfo {
    pub fn write_quorum(&self, quorum: usize) -> usize {
        if self.data_blocks == self.parity_blocks {
            // å¹³è¡¡é…ç½®éœ€è¦å¤šæ•°æ´¾
            return self.data_blocks + 1;
        }
        // æ ‡å‡†é…ç½®åªéœ€æ•°æ®å—æ•°é‡
        self.data_blocks
    }
}
```

**ç¤ºä¾‹**:

| é…ç½®  | Data Blocks | Parity Blocks | Write Quorum | è¯´æ˜    |
|-----|-------------|---------------|--------------|-------|
| 4+2 | 4           | 2             | 4            | æ•°æ®å—å³å¯ |
| 2+2 | 2           | 2             | 3            | éœ€è¦å¤šæ•°æ´¾ |
| 8+4 | 8           | 4             | 8            | æ•°æ®å—å³å¯ |
| 6+6 | 6           | 6             | 7            | éœ€è¦å¤šæ•°æ´¾ |

## æ•°æ®è¯»å–æµç¨‹

### å®Œæ•´è¯»å–æµç¨‹å›¾

```
Client GET Request
       â†“
[1] ECStore::get_object
       â†“
[2] é€‰æ‹© Set
       â†“
[3] SetDisks::get_object
       â†“
[4] è¯»å–å…ƒæ•°æ®ï¼ˆä»å¤šä¸ªç£ç›˜ï¼‰
       â†“
[5] åˆå¹¶å…ƒæ•°æ®ï¼ˆé€‰æ‹©æœ€æ–°/ä¸€è‡´çš„ï¼‰
       â†“
[6] åˆ›å»º readers (è‡³å°‘ data_shards ä¸ª)
       â†“                          â”Œâ”€ æˆåŠŸè¯»å–
       â”œâ”€ Disk 0 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 1 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 2 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 3 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 4 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 5 (Data)  â”€â”€Xâ”€â”€â”€â”€â”€â”¤â”€ è¯»å–å¤±è´¥ï¼ˆç£ç›˜æ•…éšœï¼‰
       â”œâ”€ Disk 6 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 7 (Data)  â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”œâ”€ Disk 8 (Parity) â”€â”€â”€â”€â”€â”€â”€â”¼â”€ ä½¿ç”¨æ ¡éªŒå—è¡¥å¿
       â”œâ”€ Disk 9 (Parity) â”€â”€Xâ”€â”€â”€â”€â”¤â”€ è¯»å–å¤±è´¥
       â”œâ”€ Disk 10 (Parity) â”€â”€â”€â”€â”€â”€â”¤
       â””â”€ Disk 11 (Parity) â”€â”€Xâ”€â”€â”€â”˜â”€ æœªä½¿ç”¨
       â†“
[7] Erasure è§£ç ï¼ˆä»å¯ç”¨åˆ†ç‰‡æ¢å¤æ•°æ®ï¼‰
       â†“
[8] æµå¼è¾“å‡ºåˆ°å®¢æˆ·ç«¯
       â†“
[9] è¿”å›æ•°æ®
```

### å…³é”®æ­¥éª¤è¯¦è§£

#### æ­¥éª¤ 4-5: å…ƒæ•°æ®è¯»å–ä¸åˆå¹¶

```rust
// ä½ç½®ï¼šcrates/ecstore/src/set_disk.rs

async fn get_object<W>(&self, bucket: &str, object: &str, writer: W, ...) -> Result<()>
where
    W: AsyncWrite + Send + Sync + Unpin + 'static,
{
    let disks = self.disks.read().await;
    
    // 4. ä»æ‰€æœ‰å¯ç”¨ç£ç›˜è¯»å–å…ƒæ•°æ®
    let (files, errs) = self.read_all_file_info(bucket, object, &disks).await;

    // 5. é€‰æ‹©æœ‰æ•ˆçš„æ–‡ä»¶ä¿¡æ¯
    let fi = if files.is_empty() {
        return Err(Error::ObjectNotFound);
    } else {
        // é€‰æ‹©æœ€æ–°çš„ã€ä¸€è‡´çš„å…ƒæ•°æ®
        self.pick_valid_file_info(&files)?
    };

    // ç»§ç»­å¤„ç†...
}
```

**å…ƒæ•°æ®è¯»å–**:

```rust
async fn read_all_file_info(
    &self,
    bucket: &str,
    object: &str,
    disks: &[Option<DiskStore>],
) -> (Vec<FileInfo>, Vec<Option<Error>>) {
    let mut futures = Vec::with_capacity(disks.len());

    for disk in disks.iter() {
        futures.push(async move {
            if let Some(disk) = disk && disk.is_online().await {
                disk.read_metadata(bucket, object).await
            } else {
                Err(DiskError::DiskNotFound.into())
            }
        });
    }

    let results = join_all(futures).await;
    
    let mut files = Vec::new();
    let mut errs = Vec::new();

    for result in results {
        match result {
            Ok(fi) => {
                files.push(fi);
                errs.push(None);
            }
            Err(e) => {
                errs.push(Some(e));
            }
        }
    }

    (files, errs)
}
```

**å…ƒæ•°æ®åˆå¹¶ç­–ç•¥**:

```rust
fn pick_valid_file_info(&self, files: &[FileInfo]) -> Result<FileInfo> {
    // 1. è¿‡æ»¤æœ‰æ•ˆçš„å…ƒæ•°æ®ï¼ˆæ ¡éªŒå’Œæ­£ç¡®ï¼‰
    let valid_files: Vec<_> = files
        .iter()
        .filter(|fi| fi.is_valid())
        .collect();

    if valid_files.is_empty() {
        return Err(Error::CorruptedMetadata);
    }

    // 2. æŒ‰ ModTime æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
    let latest = valid_files
        .iter()
        .max_by_key(|fi| fi.mod_time)
        .unwrap();

    // 3. éªŒè¯ä¸€è‡´æ€§ï¼ˆè‡³å°‘ read_quorum ä¸ªç›¸åŒï¼‰
    let count = valid_files
        .iter()
        .filter(|fi| fi.data_dir == latest.data_dir)
        .count();

    if count >= self.format.erasure.data_blocks {
        Ok((*latest).clone())
    } else {
        Err(Error::InconsistentMetadata)
    }
}
```

#### æ­¥éª¤ 6-7: æ•°æ®è¯»å–ä¸è§£ç 

```rust
async fn get_object_with_fileinfo<W>(
    bucket: &str,
    object: &str,
    offset: usize,
    length: i64,
    writer: &mut W,
    fi: FileInfo,
    files: Vec<FileInfo>,
    disks: &[Option<DiskStore>],
    set_index: usize,
    pool_index: usize,
) -> Result<()>
where
    W: AsyncWrite + Send + Sync + Unpin + 'static,
{
    // 6. åˆ›å»º erasure è§£ç å™¨
    let erasure = erasure_coding::Erasure::new(
        fi.erasure.data_blocks,
        fi.erasure.parity_blocks,
        fi.erasure.block_size,
    );

    // è®¡ç®—éœ€è¦è¯»å–çš„éƒ¨åˆ†
    let part_indices = calculate_part_indices(offset, length, &fi);

    // ä¸ºæ¯ä¸ªéƒ¨åˆ†åˆ›å»º readers
    for part_index in part_indices {
        let mut readers = Vec::with_capacity(disks.len());

        for (i, disk) in disks.iter().enumerate() {
            if let Some(disk) = disk && disk.is_online().await {
                // è¯»å–å¯¹åº”çš„åˆ†ç‰‡æ–‡ä»¶
                let shard_path = format!(
                    "{}/{}/part.{}",
                    fi.data_dir.unwrap(),
                    part_index,
                    files[i].erasure.index
                );
                
                match disk.read_file(bucket, &shard_path).await {
                    Ok(reader) => readers.push(Some(reader)),
                    Err(_) => readers.push(None),
                }
            } else {
                readers.push(None);
            }
        }

        // 7. è§£ç å¹¶å†™å…¥è¾“å‡º
        let (written, err) = erasure
            .decode(writer, readers, part_offset, part_length, part_size)
            .await;

        if let Some(e) = err {
            error!("Failed to decode part {}: {:?}", part_index, e);
            return Err(e);
        }
    }

    Ok(())
}
```

**è§£ç å®ç°** (`crates/ecstore/src/erasure_coding/decode.rs`):

```rust
pub async fn decode<W>(
    &self,
    writer: &mut W,
    mut readers: Vec<Option<Box<dyn AsyncRead + Send + Unpin>>>,
    offset: usize,
    length: usize,
    total_size: usize,
) -> (usize, Option<Error>)
where
    W: AsyncWrite + Send + Sync + Unpin,
{
    // æ£€æŸ¥å¯ç”¨çš„ readers
    let available_count = readers.iter().filter(|r| r.is_some()).count();
    if available_count < self.data_shards {
        return (0, Some(Error::InsufficientReadQuorum));
    }

    let mut total_written = 0;
    let mut buffer = vec![vec![0u8; self.block_size]; self.data_shards + self.parity_shards];

    loop {
        // ä»æ¯ä¸ª reader è¯»å–ä¸€ä¸ªå—
        let mut shards = Vec::with_capacity(readers.len());
        for (i, reader_opt) in readers.iter_mut().enumerate() {
            if let Some(reader) = reader_opt {
                match read_exact(reader, &mut buffer[i]).await {
                    Ok(_) => shards.push(Some(&buffer[i][..])),
                    Err(_) => shards.push(None),
                }
            } else {
                shards.push(None);
            }
        }

        // Reed-Solomon è§£ç 
        if let Err(e) = self.reconstruct_data(&mut shards) {
            return (total_written, Some(e));
        }

        // å†™å…¥è§£ç åçš„æ•°æ®å—
        for i in 0..self.data_shards {
            if let Some(data) = shards[i] {
                let write_len = std::cmp::min(data.len(), length - total_written);
                if write_len == 0 {
                    break;
                }

                if let Err(e) = writer.write_all(&data[..write_len]).await {
                    return (total_written, Some(e.into()));
                }

                total_written += write_len;
            }
        }

        if total_written >= length {
            break;
        }
    }

    (total_written, None)
}

fn reconstruct_data(&self, shards: &mut [Option<&[u8]>]) -> Result<()> {
    let encoder = self.encoder.as_ref().unwrap();
    
    // ä½¿ç”¨ reed-solomon-simd åº“è§£ç 
    encoder.reconstruct(shards)?;
    
    Ok(())
}
```

### è¯»å–ä»²è£è§„åˆ™

```rust
// æœ€å°‘éœ€è¦ data_shards ä¸ªå¯ç”¨åˆ†ç‰‡
if available_readers < erasure.data_shards {
    return Err(Error::InsufficientReadQuorum);
}

// å¯ä»¥ä»ä»»æ„ data_shards ä¸ªåˆ†ç‰‡ä¸­æ¢å¤
// ä¾‹å¦‚ï¼š8+4 é…ç½®ï¼Œ12 ä¸ªåˆ†ç‰‡ä¸­ä»»æ„ 8 ä¸ªå³å¯
```

**ä¼˜åŒ–ç­–ç•¥**:

1. **ä¼˜å…ˆè¯»å–æ•°æ®å—**: é¿å…ä¸å¿…è¦çš„è§£ç è®¡ç®—
2. **å¹¶è¡Œè¯»å–**: åŒæ—¶ä»å¤šä¸ªç£ç›˜è¯»å–
3. **å¿«é€Ÿå¤±è´¥**: è¶…æ—¶çš„ reader ç«‹å³è·³è¿‡
4. **æ™ºèƒ½é€‰æ‹©**: é€‰æ‹©å“åº”æœ€å¿«çš„åˆ†ç‰‡ç»„åˆ

## åˆ†å¸ƒå¼åè°ƒ

### èŠ‚ç‚¹å‘ç°ä¸é€šä¿¡

#### Peer S3 Client

**ä½ç½®**: `crates/ecstore/src/rpc/peer_s3_client.rs`

```rust
pub struct S3PeerSys {
    peers: HashMap<String, Arc<PeerClient>>,  // endpoint -> client
}

pub struct PeerClient {
    endpoint: String,
    client: reqwest::Client,
    local: bool,  // æ˜¯å¦æœ¬åœ°èŠ‚ç‚¹
}

impl S3PeerSys {
    pub fn new(endpoint_pools: &EndpointServerPools) -> Self {
        let mut peers = HashMap::new();

        for pool in endpoint_pools.as_ref() {
            for ep in &pool.endpoints {
                let endpoint = format!("{}://{}:{}", ep.scheme, ep.host, ep.port);
                let client = PeerClient {
                    endpoint: endpoint.clone(),
                    client: reqwest::Client::new(),
                    local: ep.is_local,
                };
                peers.insert(endpoint, Arc::new(client));
            }
        }

        S3PeerSys { peers }
    }

    // RPC è°ƒç”¨è¿œç¨‹èŠ‚ç‚¹
    pub async fn call_peer(
        &self,
        endpoint: &str,
        method: &str,
        path: &str,
        body: Option<Bytes>,
    ) -> Result<Bytes> {
        let peer = self.peers.get(endpoint)
            .ok_or(Error::PeerNotFound)?;

        if peer.local {
            // æœ¬åœ°è°ƒç”¨ï¼ˆç›´æ¥å‡½æ•°è°ƒç”¨ï¼‰
            return self.local_call(method, path, body).await;
        }

        // è¿œç¨‹è°ƒç”¨ï¼ˆHTTP RPCï¼‰
        let url = format!("{}{}", peer.endpoint, path);
        let response = peer.client
            .request(method.parse().unwrap(), &url)
            .body(body.unwrap_or_default())
            .send()
            .await?;

        response.bytes().await.map_err(Into::into)
    }
}
```

### åˆ†å¸ƒå¼é”

**ä½ç½®**: `crates/lock/`

```rust
pub trait LockClient: Send + Sync {
    async fn get_lock(&self, name: &str, duration: Duration) -> Result<Lock>;
    async fn get_write_lock(&self, timeout: Duration) -> Result<WriteGuard>;
    async fn get_read_lock(&self, timeout: Duration) -> Result<ReadGuard>;
}

// æœ¬åœ°å®ç°ï¼ˆå•èŠ‚ç‚¹ï¼‰
pub struct LocalClient {
    locks: Arc<RwLock<HashMap<String, Arc<RwLock<()>>>>>,
}

// åˆ†å¸ƒå¼å®ç°ï¼ˆå¤šèŠ‚ç‚¹ï¼‰
pub struct DistributedClient {
    endpoints: Vec<String>,
    clients: Vec<Arc<dyn LockClient>>,
}
```

**ä½¿ç”¨ç¤ºä¾‹**:

```rust
// å†™å¯¹è±¡å‰è·å–é”
let ns_lock = self.new_ns_lock(bucket, object).await?;
let _guard = ns_lock
    .get_write_lock(get_lock_acquire_timeout())
    .await?;

// æ‰§è¡Œå†™æ“ä½œ
self.put_object_internal(bucket, object, data, opts).await?;

// guard è¢« drop æ—¶è‡ªåŠ¨é‡Šæ”¾é”
```

### ä¸€è‡´æ€§ä¿è¯

#### å†™å…¥ä¸€è‡´æ€§

```rust
// å†™å…¥ä»²è£ä¿è¯ï¼šè‡³å°‘ data_blocks ä¸ªç£ç›˜å†™å…¥æˆåŠŸ
if success_count >= write_quorum {
    // æäº¤å…ƒæ•°æ®
    commit_metadata().await?;
} else {
    // å›æ»šï¼šåˆ é™¤å·²å†™å…¥çš„æ•°æ®
    rollback().await?;
    return Err(Error::InsufficientWriteQuorum);
}
```

#### è¯»å–ä¸€è‡´æ€§

```rust
// 1. ä»å¤šä¸ªç£ç›˜è¯»å–å…ƒæ•°æ®
let files = read_all_metadata().await?;

// 2. é€‰æ‹©ä¸€è‡´çš„å…ƒæ•°æ®ï¼ˆè‡³å°‘ read_quorum ä¸ªç›¸åŒï¼‰
let fi = pick_consistent_metadata(&files)?;

// 3. éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆæ ¡éªŒå’Œï¼‰
verify_checksum(&fi, &data)?;
```

#### å…ƒæ•°æ®ç‰ˆæœ¬æ§åˆ¶

```rust
pub struct FileInfo {
    pub version_id: Option<Uuid>,    // å¯¹è±¡ç‰ˆæœ¬
    pub data_dir: Option<Uuid>,      // æ•°æ®ç›®å½• UUID
    pub mod_time: OffsetDateTime,    // ä¿®æ”¹æ—¶é—´
    pub erasure: ErasureInfo,        // æ“¦é™¤ç¼–ç ä¿¡æ¯
    // ...
}

// å†²çªè§£å†³ï¼šé€‰æ‹©æœ€æ–°çš„ç‰ˆæœ¬
impl FileInfo {
    pub fn is_newer_than(&self, other: &FileInfo) -> bool {
        self.mod_time > other.mod_time
    }
}
```

## æ•…éšœæ¢å¤

### è‡ªåŠ¨æ£€æµ‹

#### Data Scanner

**ä½ç½®**: `crates/scanner/`

```rust
pub struct DataScanner {
    store: Arc<ECStore>,
    cancel_token: CancellationToken,
}

impl DataScanner {
    pub async fn start(&self) {
        loop {
            tokio::select! {
                _ = self.cancel_token.cancelled() => break,
                _ = tokio::time::sleep(Duration::from_secs(3600)) => {
                    // æ¯å°æ—¶æ‰«æä¸€æ¬¡
                    self.scan_all_buckets().await;
                }
            }
        }
    }

    async fn scan_all_buckets(&self) {
        let buckets = self.store.list_buckets().await.unwrap();
        
        for bucket in buckets {
            self.scan_bucket(&bucket.name).await;
        }
    }

    async fn scan_bucket(&self, bucket: &str) {
        // éå†æ‰€æœ‰å¯¹è±¡
        let objects = self.store.list_objects(bucket, None).await.unwrap();
        
        for obj in objects {
            // æ£€æŸ¥æ¯ä¸ªå¯¹è±¡çš„æ‰€æœ‰åˆ†ç‰‡
            if let Err(e) = self.verify_object(bucket, &obj.key).await {
                warn!("Object {}/{} verification failed: {:?}", bucket, obj.key, e);
                
                // è§¦å‘ä¿®å¤
                self.heal_object(bucket, &obj.key).await;
            }
        }
    }

    async fn verify_object(&self, bucket: &str, key: &str) -> Result<()> {
        // 1. è¯»å–å…ƒæ•°æ®
        let fi = self.store.get_object_info(bucket, key).await?;
        
        // 2. æ£€æŸ¥æ‰€æœ‰åˆ†ç‰‡
        let disks = self.get_disks_for_object(bucket, key).await?;
        
        for (i, disk) in disks.iter().enumerate() {
            if let Some(disk) = disk {
                // æ£€æŸ¥åˆ†ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                let shard_path = format!("{}/part.{}", fi.data_dir.unwrap(), i + 1);
                
                match disk.stat(bucket, &shard_path).await {
                    Ok(_) => {
                        // éªŒè¯æ ¡éªŒå’Œ
                        if let Err(e) = disk.verify_checksum(bucket, &shard_path, &fi).await {
                            return Err(Error::CorruptedShard(i, e));
                        }
                    }
                    Err(_) => {
                        return Err(Error::MissingShard(i));
                    }
                }
            }
        }
        
        Ok(())
    }
}
```

### è‡ªåŠ¨ä¿®å¤

#### Heal Manager

**ä½ç½®**: `crates/heal/`

```rust
pub struct HealManager {
    storage: Arc<dyn HealStorage>,
    cancel_token: CancellationToken,
}

impl HealManager {
    pub async fn heal_object(&self, bucket: &str, key: &str) -> Result<HealResult> {
        // 1. è·å–å¯¹è±¡ä¿¡æ¯å’Œæ‰€æœ‰åˆ†ç‰‡çŠ¶æ€
        let (fi, shard_status) = self.storage.get_object_heal_info(bucket, key).await?;
        
        // 2. è¯†åˆ«éœ€è¦ä¿®å¤çš„åˆ†ç‰‡
        let missing_shards: Vec<usize> = shard_status
            .iter()
            .enumerate()
            .filter(|(_, status)| status.is_missing_or_corrupted())
            .map(|(i, _)| i)
            .collect();
        
        if missing_shards.is_empty() {
            return Ok(HealResult::NoActionNeeded);
        }
        
        // 3. æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¿®å¤ï¼ˆéœ€è¦è‡³å°‘ data_shards ä¸ªå®Œæ•´åˆ†ç‰‡ï¼‰
        let available_shards = shard_status.len() - missing_shards.len();
        if available_shards < fi.erasure.data_blocks {
            return Err(Error::CannotHeal("insufficient shards".into()));
        }
        
        // 4. è¯»å–å¯ç”¨åˆ†ç‰‡
        let mut available_data = Vec::with_capacity(shard_status.len());
        for (i, disk) in self.storage.get_disks(bucket, key).await?.iter().enumerate() {
            if missing_shards.contains(&i) {
                available_data.push(None);
            } else {
                let shard = disk.unwrap().read_shard(bucket, key, i).await?;
                available_data.push(Some(shard));
            }
        }
        
        // 5. é‡å»ºä¸¢å¤±çš„åˆ†ç‰‡
        let erasure = Erasure::new(
            fi.erasure.data_blocks,
            fi.erasure.parity_blocks,
            fi.erasure.block_size,
        );
        
        for missing_index in &missing_shards {
            let reconstructed = erasure.reconstruct_shard(&available_data, *missing_index)?;
            
            // 6. å†™å›ä¿®å¤åçš„åˆ†ç‰‡
            let disk = self.storage.get_disk(bucket, key, *missing_index).await?;
            disk.write_shard(bucket, key, *missing_index, reconstructed).await?;
            
            info!("Healed shard {} for object {}/{}", missing_index, bucket, key);
        }
        
        Ok(HealResult::Healed {
            repaired_shards: missing_shards,
        })
    }
}
```

**åˆ†ç‰‡é‡å»º** (`crates/ecstore/src/erasure_coding/heal.rs`):

```rust
impl Erasure {
    pub fn reconstruct_shard(
        &self,
        available_shards: &[Option<Bytes>],
        missing_index: usize,
    ) -> Result<Bytes> {
        // ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¯ç”¨åˆ†ç‰‡
        let available_count = available_shards.iter().filter(|s| s.is_some()).count();
        if available_count < self.data_shards {
            return Err(Error::InsufficientShards);
        }
        
        // å‡†å¤‡åˆ†ç‰‡æ•°æ®
        let mut shards: Vec<Option<&[u8]>> = available_shards
            .iter()
            .map(|s| s.as_ref().map(|b| b.as_ref()))
            .collect();
        
        // ä½¿ç”¨ Reed-Solomon è§£ç å™¨é‡å»º
        let encoder = self.encoder.as_ref().unwrap();
        encoder.reconstruct(&mut shards)?;
        
        // è¿”å›é‡å»ºçš„åˆ†ç‰‡
        Ok(shards[missing_index].unwrap().to_vec().into())
    }
}
```

### ç£ç›˜æ›´æ¢

```rust
// ä½ç½®ï¼šcrates/ecstore/src/set_disk.rs

impl SetDisks {
    pub async fn renew_disk(&self, ep: &Endpoint) {
        // 1. åˆ›å»ºæ–°çš„ç£ç›˜å®ä¾‹
        let new_disk = new_disk(ep, &DiskOption::default()).await.unwrap();
        
        // 2. åŠ è½½æ ¼å¼ä¿¡æ¯
        let fm = new_disk.load_format().await.unwrap();
        
        // 3. æŸ¥æ‰¾è¦æ›¿æ¢çš„ç£ç›˜ç´¢å¼•
        let (set_idx, disk_idx) = self.find_disk_index(&fm).unwrap();
        
        // 4. æ›´æ–°ç£ç›˜ ID
        new_disk.set_disk_id(Some(fm.erasure.this)).await.unwrap();
        
        // 5. æ›´æ–°å…¨å±€æ˜ å°„
        if new_disk.is_local() {
            let mut global_local_disk_map = GLOBAL_LOCAL_DISK_MAP.write().await;
            global_local_disk_map.insert(
                new_disk.endpoint().to_string(),
                Some(new_disk.clone())
            );
            
            if is_dist_erasure().await {
                let mut local_set_drives = GLOBAL_LOCAL_DISK_SET_DRIVES.write().await;
                local_set_drives[self.pool_index][set_idx][disk_idx] = Some(new_disk.clone());
            }
        }
        
        // 6. æ›¿æ¢ Set ä¸­çš„ç£ç›˜
        let mut disk_lock = self.disks.write().await;
        disk_lock[disk_idx] = Some(new_disk);
        
        info!("Disk renewed: set={}, disk={}", set_idx, disk_idx);
        
        // 7. è§¦å‘è¯¥ç£ç›˜ä¸Šæ‰€æœ‰å¯¹è±¡çš„ä¿®å¤
        self.heal_disk(disk_idx).await;
    }
}
```

## æ€§èƒ½ä¼˜åŒ–

### SIMD åŠ é€Ÿ

```rust
// ç¼–è¯‘æ—¶ç‰¹æ€§æ£€æµ‹
#[cfg(target_arch = "x86_64")]
use std::arch::x86_64::*;

#[cfg(target_arch = "aarch64")]
use std::arch::aarch64::*;

// reed-solomon-simd è‡ªåŠ¨é€‰æ‹©æœ€ä½³ SIMD å®ç°
let encoder = reed_solomon_simd::ReedSolomonEncoder::new(
    data_shards,
    parity_shards,
    shard_len,
)?;

// åœ¨æ”¯æŒçš„ CPU ä¸Šï¼š
// - Intel/AMD: AVX2 æˆ– AVX-512
// - ARM: NEON
// - å¦åˆ™å›é€€åˆ°æ ‡å‡†å®ç°
```

**æ€§èƒ½å¯¹æ¯”**:

| CPU      | æŒ‡ä»¤é›†     | ç¼–ç é€Ÿåº¦     | è§£ç é€Ÿåº¦     |
|----------|---------|----------|----------|
| Intel i7 | Scalar  | 500 MB/s | 450 MB/s |
| Intel i7 | AVX2    | 2.5 GB/s | 2.2 GB/s |
| Intel i9 | AVX-512 | 5.0 GB/s | 4.5 GB/s |
| ARM M1   | NEON    | 1.8 GB/s | 1.6 GB/s |

### å¹¶è¡Œ I/O

```rust
// å¹¶è¡Œå†™å…¥æ‰€æœ‰åˆ†ç‰‡
let futures: Vec<_> = writers
    .iter_mut()
    .zip(shards.iter())
    .map(|(writer, shard)| async move {
        if let Some(w) = writer {
            w.write_all(shard).await
        } else {
            Err(io::Error::new(io::ErrorKind::Other, "no writer"))
        }
    })
    .collect();

// ç­‰å¾…æ‰€æœ‰å†™å…¥å®Œæˆ
let results = join_all(futures).await;
```

### é›¶æ‹·è´

```rust
// ä½¿ç”¨ Bytes é¿å…æ•°æ®å¤åˆ¶
use bytes::{Bytes, BytesMut};

// ä»ç½‘ç»œè¯»å–ç›´æ¥åˆ°ç¼–ç å™¨
let buf = BytesMut::with_capacity(block_size);
reader.read_buf(&mut buf).await?;
let bytes: Bytes = buf.freeze();  // é›¶æ‹·è´è½¬æ¢

// ç¼–ç åç›´æ¥å†™å…¥ç£ç›˜
disk.write_all(bytes).await?;  // ç§»åŠ¨è¯­ä¹‰ï¼Œæ— æ‹·è´
```

### å†…å­˜æ± 

```rust
// ä½¿ç”¨å¯¹è±¡æ± å‡å°‘å†…å­˜åˆ†é…
use bytes::BytesMut;

struct BufferPool {
    pool: Vec<BytesMut>,
    size: usize,
}

impl BufferPool {
    fn acquire(&mut self) -> BytesMut {
        self.pool.pop().unwrap_or_else(|| BytesMut::with_capacity(self.size))
    }
    
    fn release(&mut self, mut buf: BytesMut) {
        buf.clear();
        if self.pool.len() < 100 {
            self.pool.push(buf);
        }
    }
}
```

### æµæ°´çº¿å¤„ç†

```rust
// è¯»å–ã€ç¼–ç ã€å†™å…¥æµæ°´çº¿
async fn pipeline_encode(
    reader: impl AsyncRead,
    writers: &mut [Writer],
    erasure: &Erasure,
) -> Result<()> {
    let (read_tx, read_rx) = mpsc::channel(4);
    let (encode_tx, encode_rx) = mpsc::channel(4);

    // Stage 1: è¯»å–
    tokio::spawn(async move {
        let mut buffer = vec![0u8; block_size];
        loop {
            let n = reader.read(&mut buffer).await?;
            if n == 0 break;
            read_tx.send(buffer[..n].to_vec()).await?;
        }
    });

    // Stage 2: ç¼–ç 
    tokio::spawn(async move {
        while let Some(data) = read_rx.recv().await {
            let shards = erasure.encode_block(&data)?;
            encode_tx.send(shards).await?;
        }
    });

    // Stage 3: å†™å…¥
    while let Some(shards) = encode_rx.recv().await {
        parallel_write(writers, &shards).await?;
    }

    Ok(())
}
```

## é…ç½®å»ºè®®

### å°è§„æ¨¡éƒ¨ç½² (å•èŠ‚ç‚¹)

```yaml
é…ç½®: 4 ç£ç›˜ï¼Œ2+2
å®¹é‡: 4 Ã— 1TB = 4TB
å¯ç”¨: 2TB (50%)
å®¹é”™: 2 å—ç£ç›˜
é€‚ç”¨: å¼€å‘ç¯å¢ƒã€å°å‹åº”ç”¨
```

### ä¸­ç­‰è§„æ¨¡éƒ¨ç½² (3-5 èŠ‚ç‚¹)

```yaml
é…ç½®: 12 ç£ç›˜ï¼Œ8+4
å®¹é‡: 12 Ã— 4TB = 48TB
å¯ç”¨: 32TB (66.7%)
å®¹é”™: 4 å—ç£ç›˜
é€‚ç”¨: ä¼ä¸šåº”ç”¨ã€æ•°æ®å¤‡ä»½
```

### å¤§è§„æ¨¡éƒ¨ç½² (10+ èŠ‚ç‚¹)

```yaml
é…ç½®: 16 ç£ç›˜ï¼Œ12+4
å®¹é‡: 16 Ã— 10TB = 160TB
å¯ç”¨: 120TB (75%)
å®¹é”™: 4 å—ç£ç›˜
é€‚ç”¨: å¤§æ•°æ®ã€AI è®­ç»ƒã€è§†é¢‘å­˜å‚¨
```

### æè‡´å¯é æ€§

```yaml
é…ç½®: 12 ç£ç›˜ï¼Œ6+6
å®¹é‡: 12 Ã— 8TB = 96TB
å¯ç”¨: 48TB (50%)
å®¹é”™: 6 å—ç£ç›˜
é€‚ç”¨: é‡‘èã€åŒ»ç–—ã€æ”¿åºœ
```

## æ€»ç»“

RustFS çš„å¤šæœºå¤šç›˜æ•°æ®å‰¯æœ¬å¤„ç†é‡‡ç”¨äº†ç°ä»£åŒ–çš„æ“¦é™¤ç¼–ç æŠ€æœ¯ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„å¤šå‰¯æœ¬æ–¹å¼å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. **æ›´é«˜çš„å­˜å‚¨æ•ˆç‡**: 66%-75% vs 33% (3 å‰¯æœ¬)
2. **çµæ´»çš„å®¹é”™é…ç½®**: å¯æ ¹æ®éœ€æ±‚è°ƒæ•´æ•°æ®/æ ¡éªŒæ¯”ä¾‹
3. **è‡ªåŠ¨æ•…éšœæ¢å¤**: åå°æŒç»­æ‰«æå’Œä¿®å¤
4. **é«˜æ€§èƒ½**: SIMD ä¼˜åŒ– + å¹¶è¡Œ I/O
5. **åˆ†å¸ƒå¼æ¶æ„**: æ”¯æŒè·¨èŠ‚ç‚¹æ•°æ®åˆ†å¸ƒ
6. **æ•°æ®ä¸€è‡´æ€§**: åˆ†å¸ƒå¼é” + å…ƒæ•°æ®ç‰ˆæœ¬æ§åˆ¶

è¿™ç§æ¶æ„ä½¿ RustFS èƒ½å¤Ÿåœ¨ä¿è¯æ•°æ®å¯é æ€§çš„åŒæ—¶ï¼Œæä¾›å‡ºè‰²çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æ›´æ–°æ—¥æœŸ**: 2026-02-24  
**ä½œè€…**: AI Assistant

